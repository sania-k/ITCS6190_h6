# Music Streaming Analysis Using Spark Structured APIs

## Overview
This project uses Spark Strucutred API's to gain insights into genre preferences, song popularity, and listener engagement

## Dataset Description
There are two datasets, `listening_logs.csv` and `songs_metadata.csv`. They are both randomly generated by running `datagen.py` as described in the execution intructions. The csv data is formatted as follows:

`listening_logs.csv`
| Column Name   | Description                                                        |
| ------------- | ------------------------------------------------------------------ |
| user\_id      | Unique ID of the user                                              |
| song\_id      | Unique ID of the song                                              |
| timestamp     | Date and time when the song was played (e.g., 2025-03-23 14:05:00) |
| duration\_sec | Duration in seconds for which the song was played                  |

`songs_metadata.csv`
| Column Name | Description                                                    |
| ----------- | -------------------------------------------------------------- |
| song\_id    | Unique ID of the song                                          |
| title       | Title of the song                                              |
| artist      | Name of the artist                                             |
| genre       | Genre of the song (e.g., Pop, Rock, Jazz)                      |
| mood        | Mood category of the song (e.g., Happy, Sad, Energetic, Chill) |


## Repository Structure
```
ITCS6190_h6/
├── input/
│   ├── listening_logs.csv            # Randomly generated dataset on users and the songs they listen to
│   └── songs_metadata.csv            # Randomly generated dataset on the songs in the database
├── output/
│   ├── avg_listen_time_per_song/     # Output files for most listened-to genre for each user
│   ├── genre_loyalty_scores/         # Output files for the average duration (in seconds) for each song
│   ├── night_owl_users/              # Output files for users with a loyalty score above 0.8 to their favorite genre
│   └── user_favorite_genres/         # Output files for users who frequently listen to music between 12 AM and 5 AM
├── datagen.py             # Optional data generation script
├── main.py                # Spark analysis script
├── README.md              # Project overview and instructions
├── Requirements           # Python dependencies
└── .gitignore
```

## Output Directory Structure
The output/ directory contains the results generated by the Spark analysis (main.py). It is organized into subdirectories for each type of analysis:
- `avg_listen_time_per_song/` – Stores output for the most listened-to genre per user.
   - `_SUCCESS` – Spark job success indicator.
   - `._SUCCESS.crc` – Checksum for Spark job completion.
   - `part-xxxxx.csv` – Actual output CSV files.
   - `part-xxxxx.csv.crc` – checksum for the actual CSV output file.
- `genre_loyalty_scores/` – Stores average duration (in seconds) per song for each user.
   - `_SUCCESS`, `._SUCCESS.crc`, `part-xxxxx.csv`, `part-xxxxx.csv.crc`
- `night_owl_users/` – Contains users who frequently listen to music between 12 AM and 5 AM.
   - `_SUCCESS`, `._SUCCESS.crc`, `part-xxxxx.csv`, `part-xxxxx.csv.crc`
- `user_favorite_genres/` – Contains users with a loyalty score above 0.8 for their favorite genre.
   - `_SUCCESS`, `._SUCCESS.crc`, `part-xxxxx.csv`, `part-xxxxx.csv.crc`

Each subdirectory follows the Spark output convention, where _SUCCESS and ._SUCCESS.crc indicate successful completion, and the part-xxxxx.csv files contain the actual results.

## Tasks and Outputs
This project foes the following tasks and outputs the results to the output files described above
1. Find each user’s favourite genre:
   - Identify the most listened-to genre for each user by counting how many times they played songs in each genre.
   - Outputs to `avg_listen_time_per_song/`
3. Calculate the average listen time per song:
   - Compute the average duration (in seconds) for each song based on user play history.
   - Outputs to `genre_loyalty_scores/`
5. Compute the genre loyalty score for each user:
   - For each user, calculate the proportion of their plays that belong to their most-listened genre. Output users with a loyalty score above 0.8.
   - Outputs to `night_owl_users/`
7. Identify users who listen to music between 12 AM and 5 AM:
   - Extract users who frequently listen to music between 12 AM and 5 AM based on their listening timestamps.
   - Outputs to `user_favorite_genres/`

## Execution Instructions
### 1. *Prerequisites*

Before starting the assignment, ensure you have the following software installed and properly configured on your machine:

1. *Python 3.x*:
   - [Download and Install Python](https://www.python.org/downloads/)
   - Verify installation:
     ```bash
     python3 --version
     ```

2. *PySpark*:
   - Install using pip:
     ```bash
     pip install pyspark
     ```

3. *Apache Spark*:
   - Ensure Spark is installed. You can download it from the [Apache Spark Downloads](https://spark.apache.org/downloads.html) page.
   - Verify installation by running:
     ```bash
     spark-submit --version
     ```

### *2. Running the Analysis Tasks*

####  *Running Locally*

1. *Generate the Input*:
  ```bash
   python3 datagen.py
   ```

2. **Execute Each Task Using spark-submit**:
   ```bash
     spark-submit main.py
   ```

3. *Verify the Outputs*:
   Check the outputs/ directory for the resulting files:
   ```bash
   ls outputs/
   ```

## Errors and Resolutions
When setting up this project I ran into errors stemming from dependancy issues between Java, Python, Hadoop, and Apache Spark. This was solved by using a virtual environment that used all the right dependancies, as well as fixing my system environmental variables to point to the right folders. In particular:
- If you receive a `ModuleNotFoundError: No module named 'pyspark'` error while running 2.2. after installing PySpark, check to make sure that you are using the correct Python environment. To avoid confusion, you can set up a python virtual environment by running the following in your terminal when in the repository folder:
  1. Create a virtual environment called venv:
     ```
     python -m venv venv
     ```
  3. Activate the venv environment:
     ```
     .\venv\Scripts\activate
     ```
  5. Reinstall PySpark into the venv:
     ```
     pip install pyspark
     ```
  7. To deactivate the virtual environment when you are complete, execute:
     ```
     deactivate
     ```
- If you recieve a `PermissionError: [WinError 5] Access is denied` error, confirm you are using the appropriate version of Java for your Spark download. You can check this by running `java -version`. If the version is incorrect, navigate to your system's environmental variables and confirm they are pointed in the right direction
After confirming these, be sure to restart your terminal and retry running the analysis tasks.
<br><br>
In this project, I learnt the basics of using PySpark to run Spark Strucutred APIs to gain insights in a similar way to how it would be done in SQL.
